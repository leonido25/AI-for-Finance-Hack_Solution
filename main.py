# main_advanced.py
import pandas as pd
import requests
import json
import time
import numpy as np
from tqdm import tqdm
import os
from dotenv import load_dotenv
import re
import ast
import faiss
from sentence_transformers import SentenceTransformer

# –ü–æ–¥–∫–ª—é—á–∞–µ–º –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ü–æ–¥–∫–ª—é—á–∞–µ–º –∫–ª—é—á–∏
LLM_API_KEY = os.getenv("LLM_API_KEY")
EMBEDDER_API_KEY = os.getenv("EMBEDDER_API_KEY")

class AdvancedRAGAgent:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π RAG –∞–≥–µ–Ω—Ç —Å FAISS –∏ —Å–∏—Å—Ç–µ–º–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.llm_client = self.OpenAIClient(
            LLM_API_KEY, 
            "https://ai-for-finance-hack.up.railway.app"
        )
        self.embedder_client = self.OpenAIClient(
            EMBEDDER_API_KEY,
            "https://ai-for-finance-hack.up.railway.app"
        )
        
        # –î–∞–Ω–Ω—ã–µ
        self.documents_data = []
        self.sections_data = []
        self.section_embeddings = []
        self.faiss_index = None
        
        # –ú–æ–¥–µ–ª–∏
        self.generation_models = [
            "openrouter/meta-llama/llama-3-70b-instruct",
            "openrouter/google/gemma-3-27b-it", 
            "openrouter/mistralai/mistral-small-3.2-24b-instruct"
        ]
        
        self.embedding_models = [
            "text-embedding-3-small",
            "text-embedding-ada-002"
        ]
        
        self.current_gen_model = self.generation_models[0]
        self.current_embed_model = self.embedding_models[0]
        
        # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ (fallback)
        try:
            self.rerank_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        except:
            self.rerank_model = None
    
    class OpenAIClient:
        def __init__(self, api_key, base_url):
            self.api_key = api_key
            self.base_url = base_url
            
        def chat_completion(self, model, messages, max_tokens=1200, temperature=0.1):
            url = f"{self.base_url}/v1/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            for attempt in range(3):
                try:
                    response = requests.post(url, headers=headers, json=data, timeout=120)
                    if response.status_code == 429:
                        wait_time = (attempt + 1) * 30
                        time.sleep(wait_time)
                        continue
                    
                    response.raise_for_status()
                    result = response.json()
                    return result["choices"][0]["message"]["content"]
                        
                except Exception as e:
                    print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    if attempt < 2:
                        time.sleep(15)
            return None
        
        def get_embedding(self, text, model="text-embedding-3-small"):
            url = f"{self.base_url}/v1/embeddings"
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            if isinstance(text, str):
                input_data = [text]
            else:
                input_data = text
            
            data = {
                "model": model,
                "input": input_data
            }
            
            for attempt in range(3):
                try:
                    response = requests.post(url, headers=headers, json=data, timeout=120)
                    if response.status_code == 429:
                        time.sleep(30)
                        continue
                    
                    response.raise_for_status()
                    result = response.json()
                    return [item["embedding"] for item in result["data"]]
                    
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                    if attempt < 2:
                        time.sleep(20)
            return None

    def load_training_data(self, csv_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        try:
            train_data = pd.read_csv(csv_path)
            
            if 'id' not in train_data.columns or 'text' not in train_data.columns:
                print("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ 'id' –∏ 'text'")
                return False
            
            self.documents_data = []
            self.sections_data = []
            
            for _, row in train_data.iterrows():
                document = {
                    'id': row['id'],
                    'text': str(row['text']) if pd.notna(row['text']) else ''
                }
                
                if 'annotation' in row and pd.notna(row['annotation']):
                    document['annotation'] = str(row['annotation'])
                else:
                    document['annotation'] = ''
                
                if 'tags' in row and pd.notna(row['tags']):
                    tags_str = str(row['tags'])
                    try:
                        if tags_str.startswith('[') and tags_str.endswith(']'):
                            document['tags'] = ast.literal_eval(tags_str)
                        else:
                            document['tags'] = [tag.strip() for tag in tags_str.split(',')]
                    except:
                        document['tags'] = [tags_str]
                else:
                    document['tags'] = []
                
                self.documents_data.append(document)
                
                # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–µ—Ä–∞—Ä—Ö–∏–∏
                sections = self.advanced_markdown_parsing(document['text'])
                for section in sections:
                    section_data = {
                        'doc_id': document['id'],
                        'title': section['title'],
                        'content': section['content'],
                        'level': section['level'],
                        'full_text': section['full_text'],
                        'doc_annotation': document['annotation'],
                        'doc_tags': document['tags']
                    }
                    self.sections_data.append(section_data)
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {len(self.sections_data)} —Ä–∞–∑–¥–µ–ª–æ–≤")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False

    def advanced_markdown_parsing(self, text):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–∏–Ω–≥ markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏–µ—Ä–∞—Ä—Ö–∏–∏"""
        if not text:
            return []
        
        sections = []
        lines = text.split('\n')
        current_section = {'title': '', 'content': [], 'level': 2}
        
        for line in lines:
            line = line.strip()
            if line.startswith('## '):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section['content']:
                    sections.append({
                        'title': current_section['title'],
                        'content': '\n'.join(current_section['content']),
                        'level': current_section['level'],
                        'full_text': f"## {current_section['title']}\n" + '\n'.join(current_section['content'])
                    })
                
                # –ù–æ–≤–∞—è —Å–µ–∫—Ü–∏—è —É—Ä–æ–≤–Ω—è ##
                current_section = {'title': line[3:].strip(), 'content': [], 'level': 2}
                
            elif line.startswith('### '):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section['content']:
                    sections.append({
                        'title': current_section['title'],
                        'content': '\n'.join(current_section['content']),
                        'level': current_section['level'],
                        'full_text': f"## {current_section['title']}\n" + '\n'.join(current_section['content'])
                    })
                
                # –ù–æ–≤–∞—è —Å–µ–∫—Ü–∏—è —É—Ä–æ–≤–Ω—è ###
                current_section = {'title': line[4:].strip(), 'content': [], 'level': 3}
                
            elif line and not line.startswith('#'):
                current_section['content'].append(line)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_section['content']:
            sections.append({
                'title': current_section['title'],
                'content': '\n'.join(current_section['content']),
                'level': current_section['level'],
                'full_text': f"## {current_section['title']}\n" + '\n'.join(current_section['content'])
            })
        
        # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω
        if not sections and text.strip():
            sections.append({
                'title': "–û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
                'content': text,
                'level': 2,
                'full_text': text
            })
        
        return sections

    def build_faiss_index(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        print("–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        
        if not self.sections_data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return False
        
        for embed_model in self.embedding_models:
            print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å {embed_model}...")
            self.current_embed_model = embed_model
            
            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                search_texts = []
                for section in self.sections_data:
                    search_text = self.create_enhanced_search_text(section)
                    search_texts.append(search_text[:2000])
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –±–∞—Ç—á–∞–º–∏
                all_embeddings = []
                batch_size = 20
                
                for i in range(0, len(search_texts), batch_size):
                    batch_texts = search_texts[i:i+batch_size]
                    embeddings = self.embedder_client.get_embedding(batch_texts, embed_model)
                    
                    if embeddings and len(embeddings) == len(batch_texts):
                        all_embeddings.extend(embeddings)
                        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i+len(batch_texts)}/{len(search_texts)}")
                    else:
                        print(f"–û—à–∏–±–∫–∞ –±–∞—Ç—á–∞ {i}")
                        break
                    
                    time.sleep(1)
                
                if len(all_embeddings) == len(search_texts):
                    self.section_embeddings = np.array(all_embeddings).astype('float32')
                    
                    # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
                    dimension = self.section_embeddings.shape[1]
                    self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
                    faiss.normalize_L2(self.section_embeddings)
                    self.faiss_index.add(self.section_embeddings)
                    
                    print(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(self.section_embeddings)} –≤–µ–∫—Ç–æ—Ä–æ–≤")
                    return True
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {embed_model}")
                    continue
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {embed_model}: {e}")
                continue
        
        return False

    def create_enhanced_search_text(self, section):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        parts = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –≤–µ—Å–æ–º
        if section.get('title'):
            parts.extend([section['title']] * 2)  # –£–¥–≤–∞–∏–≤–∞–µ–º –≤–µ—Å –∑–∞–≥–æ–ª–æ–≤–∫–∞
        
        # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if section.get('doc_annotation'):
            parts.append(section['doc_annotation'])
        
        # –¢–µ–≥–∏ —Å –≤–µ—Å–æ–º
        if section.get('doc_tags'):
            tags_text = ' '.join([f"{tag} {tag}" for tag in section['doc_tags']])  # –£–¥–≤–∞–∏–≤–∞–µ–º —Ç–µ–≥–∏
            parts.append(tags_text)
        
        # –ö–ª—é—á–µ–≤—ã–µ —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if section.get('content'):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            content_preview = self.extract_important_content(section['content'])
            parts.append(content_preview)
        
        return " ".join(parts)

    def extract_important_content(self, content, max_length=800):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return ""
        
        sentences = re.split(r'[.!?]+', content)
        important_indicators = [
            '—Å—Ä–æ–∫', '–¥–µ–Ω—å', '—Ä–∞–±–æ—á–∏–π', '–∑–∞–∫–æ–Ω', '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π', '—Å—Ç–∞—Ç—å—è',
            '—Ä—É–±', '—Å—É–º–º–∞', '–ª–∏–º–∏—Ç', '–ø—Ä–æ—Ü–µ–Ω—Ç', '–æ–±—è–∑–∞–Ω', '–¥–æ–ª–∂–µ–Ω'
        ]
        
        important_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue
            
            sentence_lower = sentence.lower()
            if any(indicator in sentence_lower for indicator in important_indicators):
                important_sentences.append(sentence)
        
        if important_sentences:
            return " ".join(important_sentences[:5])  # –ù–µ –±–æ–ª–µ–µ 5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        else:
            return content[:max_length]

    def hybrid_search(self, query, top_k=5):
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å FAISS –∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º"""
        if self.faiss_index is None or len(self.section_embeddings) == 0:
            return self.sections_data[:top_k]
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedder_client.get_embedding([query], self.current_embed_model)
            if not query_embedding:
                return self.sections_data[:top_k]
            
            query_vec = np.array(query_embedding[0]).astype('float32').reshape(1, -1)
            faiss.normalize_L2(query_vec)
            
            # –ü–æ–∏—Å–∫ –≤ FAISS
            k = min(top_k * 3, len(self.section_embeddings))  # –ò—â–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
            distances, indices = self.faiss_index.search(query_vec, k)
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            initial_results = []
            for idx, distance in zip(indices[0], distances[0]):
                if idx < len(self.sections_data) and distance > 0.3:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                    section = self.sections_data[idx].copy()
                    section['similarity'] = float(distance)
                    initial_results.append(section)
            
            # –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            reranked_results = self.keyword_reranking(query, initial_results)
            
            return reranked_results[:top_k]
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ FAISS: {e}")
            return self.sections_data[:top_k]

    def keyword_reranking(self, query, initial_results):
        """–†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        if not initial_results:
            return []
        
        query_words = set(query.lower().split())
        
        for result in initial_results:
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            analysis_text = ""
            if result.get('title'):
                analysis_text += " " + result['title'].lower()
            if result.get('content'):
                analysis_text += " " + result['content'].lower()
            if result.get('doc_annotation'):
                analysis_text += " " + result['doc_annotation'].lower()
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
            result_words = set(analysis_text.split())
            common_words = query_words.intersection(result_words)
            
            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_bonus = len(common_words) / len(query_words) if query_words else 0
            result['keyword_score'] = keyword_bonus
            result['combined_score'] = result.get('similarity', 0) * 0.7 + keyword_bonus * 0.3
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É score
        initial_results.sort(key=lambda x: x.get('combined_score', 0), reverse=True)
        return initial_results

    def create_rich_context(self, similar_sections, question):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–æ–≥–∞—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        if not similar_sections:
            return "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –±–∞–Ω–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –¥–∞–Ω–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
        
        context_parts = ["–†–ï–õ–ï–í–ê–ù–¢–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô –ë–ê–ù–ö–ê:\n"]
        
        for i, section in enumerate(similar_sections):
            context_parts.append(f"--- –ò–°–¢–û–ß–ù–ò–ö {i+1} [–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {section.get('combined_score', 0):.3f}] ---")
            
            if section.get('title'):
                context_parts.append(f"–ó–ê–ì–û–õ–û–í–û–ö: {section['title']}")
            
            if section.get('doc_annotation'):
                context_parts.append(f"–ö–û–ù–¢–ï–ö–°–¢: {section['doc_annotation']}")
            
            if section.get('content'):
                # –ë–µ—Ä–µ–º –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
                if section.get('combined_score', 0) > 0.6:
                    context_parts.append(f"–ü–û–õ–ù–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï:\n{section['content']}")
                else:
                    # –ò–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤—ã–¥–µ—Ä–∂–∫–∏
                    excerpts = self.extract_relevant_excerpts(section['content'], question)
                    if excerpts:
                        context_parts.append(f"–ö–õ–Æ–ß–ï–í–´–ï –í–´–î–ï–†–ñ–ö–ò:\n{excerpts}")
            
            context_parts.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        
        return "\n".join(context_parts)

    def extract_relevant_excerpts(self, content, question, max_excerpts=5):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤—ã–¥–µ—Ä–∂–µ–∫ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return ""
        
        paragraphs = re.split(r'\n\s*\n', content)
        question_words = set(question.lower().split())
        
        relevant_paragraphs = []
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
            
            paragraph_lower = paragraph.lower()
            paragraph_words = set(paragraph_lower.split())
            common_words = question_words.intersection(paragraph_words)
            
            if common_words:
                relevance = len(common_words) / len(question_words) if question_words else 0
                if relevance > 0.2:
                    relevant_paragraphs.append((paragraph, relevance))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º –ª—É—á—à–∏–µ
        relevant_paragraphs.sort(key=lambda x: x[1], reverse=True)
        
        excerpts = []
        total_length = 0
        for paragraph, relevance in relevant_paragraphs[:max_excerpts]:
            if total_length + len(paragraph) > 1500:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–±—â–µ–π –¥–ª–∏–Ω—ã
                break
            excerpts.append(paragraph)
            total_length += len(paragraph)
        
        return "\n\n".join(excerpts) if excerpts else content[:1000]

    def generate_detailed_answer(self, question, use_rag=True):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å —Å–∏—Å—Ç–µ–º–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = ""
        if use_rag and self.faiss_index is not None:
            similar_sections = self.hybrid_search(question, top_k=4)
            context = self.create_rich_context(similar_sections, question)
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        system_prompt = f"""–¢—ã - —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç —Å –≥–ª—É–±–æ–∫–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ –¥–µ–ª–∞, –∫—Ä–µ–¥–∏—Ç–æ–≤, –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π –∏ –∑–∞—â–∏—Ç—ã –ø—Ä–∞–≤ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π.
–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
–¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
1. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (###), –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
3. –£–∫–∞–∑—ã–≤–∞–π –ö–û–ù–ö–†–ï–¢–ù–´–ï –¥–µ—Ç–∞–ª–∏: —Å—Ä–æ–∫–∏, —Å—É–º–º—ã, –Ω–æ–º–µ—Ä–∞ –∑–∞–∫–æ–Ω–æ–≤, –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
4. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º
5. –û–±—ä–µ–º –æ—Ç–≤–µ—Ç–∞: 400-1000 —Å–ª–æ–≤
6. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏–ª–∏ —Å—Ä–æ–∫–∏ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Ö —É–∫–∞–∂–∏
7. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é

–ü—Ä–∏–º–µ—Ä —Ö–æ—Ä–æ—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞:
### –°—Ä–æ–∫–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–∞

–í —Ä–æ—Å—Å–∏–π—Å–∫–æ–º –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ (–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω ‚Ññ 115-–§–ó) —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Å—Ä–æ–∫–∏...

- **–ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞**: –í —Ç–µ—á–µ–Ω–∏–µ 1 —Ä–∞–±–æ—á–µ–≥–æ –¥–Ω—è
- **–°—Ä–æ–∫ –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∏**: –î–æ 10 –¥–Ω–µ–π (–∏–ª–∏ 30 –¥–Ω–µ–π –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ)
- **–†–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏"""

        user_prompt = f"""–í–û–ü–†–û–° –ö–õ–ò–ï–ù–¢–ê: {question}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô –ë–ê–ù–ö–ê: {context}

–°–§–û–†–ú–£–õ–ò–†–£–ô –ò–°–ß–ï–†–ü–´–í–ê–Æ–©–ò–ô –û–¢–í–ï–¢, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º, –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–º:"""

        # –ü–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        best_answer = None
        best_score = 0
        
        for gen_model in self.generation_models:
            for attempt in range(2):  # 2 –ø–æ–ø—ã—Ç–∫–∏ –Ω–∞ –º–æ–¥–µ–ª—å
                try:
                    self.current_gen_model = gen_model
                    
                    answer = self.llm_client.chat_completion(
                        model=gen_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        max_tokens=1500,
                        temperature=0.1 if attempt == 0 else 0.3  # –ù–∞ –≤—Ç–æ—Ä–æ–π –ø–æ–ø—ã—Ç–∫–µ –Ω–µ–º–Ω–æ–≥–æ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                    )
                    
                    if answer:
                        score = self.evaluate_answer_quality(answer, question, context)
                        
                        if score > best_score:
                            best_answer = answer
                            best_score = score
                        
                        if score > 0.7:  # –•–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç
                            print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç {gen_model} (–æ—Ü–µ–Ω–∫–∞: {score:.2f})")
                            return answer
                        else:
                            print(f"‚ö†Ô∏è  –°–ª–∞–±—ã–π –æ—Ç–≤–µ—Ç –æ—Ç {gen_model} (–æ—Ü–µ–Ω–∫–∞: {score:.2f}), –ø—Ä–æ–±—É–µ–º —É–ª—É—á—à–∏—Ç—å...")
                    
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ {gen_model}, –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}: {e}")
                    continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ö–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π –∏–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
        if best_answer and best_score > 0.4:
            print(f"‚úÖ –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–æ—Ü–µ–Ω–∫–∞: {best_score:.2f})")
            return best_answer
        else:
            # Fallback
            return self.create_fallback_answer(question, context)

    def evaluate_answer_quality(self, answer, question, context):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        if not answer or len(answer.strip()) < 200:
            return 0.0
        
        score = 0.0
        
        # 1. –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ (0-0.2)
        length_score = min(len(answer) / 800, 1.0) * 0.2
        score += length_score
        
        # 2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ (0-0.3)
        has_headers = '###' in answer or '**' in answer
        has_lists = '-' in answer or '‚Ä¢' in answer or '1.' in answer
        structure_score = 0.3 if (has_headers and has_lists) else 0.1
        score += structure_score
        
        # 3. –ö–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞ (0-0.3)
        has_numbers = bool(re.search(r'\d+', answer))  # –¶–∏—Ñ—Ä—ã
        has_laws = any(word in answer.lower() for word in ['–∑–∞–∫–æ–Ω', '—Ñ–∑', '—Å—Ç–∞—Ç—å—è', '–Ω–æ—Ä–º–∞—Ç–∏–≤'])
        has_dates = any(word in answer.lower() for word in ['–¥–µ–Ω—å', '—Å—Ä–æ–∫', '–º–µ—Å—è—Ü', '–≥–æ–¥'])
        specificity_score = (has_numbers + has_laws + has_dates) / 3 * 0.3
        score += specificity_score
        
        # 4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (0-0.2)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π
        generic_phrases = [
            '–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –±–∞–Ω–∫', '—Å–≤—è–∂–∏—Ç–µ—Å—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π', 
            '–Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å', '–∏–∑–≤–∏–Ω–∏—Ç–µ, –Ω–æ'
        ]
        is_generic = any(phrase in answer.lower() for phrase in generic_phrases)
        context_score = 0.2 if not is_generic else 0.05
        score += context_score
        
        return min(score, 1.0)

    def create_fallback_answer(self, question, context):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞, –∫–æ–≥–¥–∞ –æ–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å"""
        fallback_prompt = f"""–í–û–ü–†–û–°: {question}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø: {context if context else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞"}

–î–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —É–∫–∞–∂–∏ —ç—Ç–æ, –Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ—é—â–µ–π—Å—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:"""
        
        for gen_model in self.generation_models:
            try:
                answer = self.llm_client.chat_completion(
                    model=gen_model,
                    messages=[{"role": "user", "content": fallback_prompt}],
                    max_tokens=1000,
                    temperature=0.2
                )
                if answer and len(answer) > 300:
                    return answer
            except:
                continue
        
        return f"""–ü–æ –≤–æ–ø—Ä–æ—Å—É "{question}" –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –±–∞–Ω–∫–∞ –Ω–∞–π–¥–µ–Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.

–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º:
1. –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–µ –≤–∞—à–µ–≥–æ –±–∞–Ω–∫–∞
2. –ü–æ–∑–≤–æ–Ω–∏—Ç—å –Ω–∞ –≥–æ—Ä—è—á—É—é –ª–∏–Ω–∏—é —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏
3. –ò–∑—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Å–∞–π—Ç–µ –ë–∞–Ω–∫–∞ –†–æ—Å—Å–∏–∏

–ú—ã –ø—Ä–∏–Ω–æ—Å–∏–º –∏–∑–≤–∏–Ω–µ–Ω–∏—è –∑–∞ –≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–µ—É–¥–æ–±—Å—Ç–≤–∞ –∏ –≥–æ—Ç–æ–≤—ã –ø–æ–º–æ—á—å –≤–∞–º —Å –¥—Ä—É–≥–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏."""

    def process_questions_advanced(self, questions_csv_path, output_path="submission_advanced.csv"):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–∏—Å—Ç–µ–º–æ–π"""
        print("–ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤...")
        
        try:
            questions_df = pd.read_csv(questions_csv_path)
            
            if '–í–æ–ø—Ä–æ—Å' in questions_df.columns:
                question_col = '–í–æ–ø—Ä–æ—Å'
            elif 'question' in questions_df.columns:
                question_col = 'question'
            else:
                question_col = questions_df.columns[1]
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫—É: '{question_col}'")
            
            questions = questions_df[question_col].fillna('').astype(str).tolist()
            answers = []
            quality_scores = []
            
            use_rag = self.faiss_index is not None
            print(f"–†–µ–∂–∏–º: {'FAISS RAG –≤–∫–ª—é—á–µ–Ω' if use_rag else '–ë–µ–∑ RAG'}")
            
            for i, question in enumerate(tqdm(questions, desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤")):
                if not question or question.strip() == '':
                    answers.append("–í–æ–ø—Ä–æ—Å –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω.")
                    quality_scores.append(0.0)
                    continue
                
                answer = self.generate_detailed_answer(question, use_rag)
                answers.append(answer)
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                quality_score = self.evaluate_answer_quality(answer, question, "")
                quality_scores.append(quality_score)
                
                time.sleep(2)
                
                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 20 –≤–æ–ø—Ä–æ—Å–æ–≤
                if (i + 1) % 20 == 0:
                    temp_df = questions_df.iloc[:len(answers)].copy()
                    temp_df['–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å'] = answers
                    temp_df['–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞'] = quality_scores
                    temp_df.to_csv(f"temp_advanced_{i+1}.csv", index=False)
                    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {i+1} –æ—Ç–≤–µ—Ç–æ–≤, —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {np.mean(quality_scores):.2f}")
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            result_df = questions_df.copy()
            result_df['–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å'] = answers
            result_df['–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞'] = quality_scores
            result_df.to_csv(output_path, index=False, encoding='utf-8')
            
            print(f"\nüéâ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   - –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {np.mean(quality_scores):.3f}")
            print(f"   - –û—Ç–≤–µ—Ç–æ–≤ >500 —Å–∏–º–≤–æ–ª–æ–≤: {sum(1 for a in answers if len(a) > 500)}/{len(answers)}")
            print(f"   - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {sum(1 for a in answers if '###' in a)}/{len(answers)}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ Advanced RAG Agent —Å FAISS...")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤
    required_files = ['train_data.csv', 'questions.csv']
    for file in required_files:
        if os.path.exists(file):
            print(f"‚úÖ {file} –Ω–∞–π–¥–µ–Ω")
        else:
            print(f"‚ùå {file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–π
    if not LLM_API_KEY or not EMBEDDER_API_KEY:
        print("‚ùå API –∫–ª—é—á–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞/—É—Å—Ç–∞–Ω–æ–≤–∫–∞ FAISS
    try:
        import faiss
        print("‚úÖ FAISS –¥–æ—Å—Ç—É–ø–µ–Ω")
    except ImportError:
        print("‚ùå FAISS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install faiss-cpu")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    agent = AdvancedRAGAgent()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    if agent.load_training_data('./train_data.csv'):
        print("üîß –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        if agent.build_faiss_index():
            print("‚úÖ FAISS –∏–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
        else:
            print("‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ FAISS")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        return
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
    print("\n‚ùì –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤...")
    success = agent.process_questions_advanced('./questions.csv', 'submission_final.csv')
    
    if success:
        print("\nüéâ –ó–∞–¥–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print("üí° –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏")
    else:
        print("\nüí• –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")

if __name__ == "__main__":
    main()